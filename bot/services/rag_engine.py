# bot/services/rag_engine.py
import os
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from typing import List

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" # "intfloat/multilingual-e5-large"  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π
# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ (–º–µ–Ω—å—à–µ): "cointegrated/LaBSE-en-ru" –∏–ª–∏ "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

class RAGEngine:
    def __init__(self, docs_path: str = "data/docs", db_path: str = "data/chroma_db"):
        self.docs_path = docs_path
        self.db_path = db_path
        self.model = SentenceTransformer(MODEL_NAME)

        # ChromaDB
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_or_create_collection("ecofes_docs")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        self._load_and_index_docs()

    def _load_and_index_docs(self):
        import glob
        import os

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª—ã
        doc_files = list(glob.glob(f"{self.docs_path}/**/*.*", recursive=True))
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –≤ {self.docs_path}: {len(doc_files)}")
        for f in doc_files:
            print(f"üìÑ {f}")

        # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ë–î ‚Äî –Ω–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º
        if self.collection.count() > 0:
            print(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç {self.collection.count()} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É.")
            return

        documents = []
        metadatas = []
        ids = []

        for file_path in doc_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read().strip()
                    if len(content) < 10:
                        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª: {file_path}")
                        continue

                    chunks = self._split_text(content, chunk_size=300)
                    print(f"‚úÇÔ∏è  –§–∞–π–ª {os.path.basename(file_path)} —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤")

                    for i, chunk in enumerate(chunks):
                        doc_id = f"{os.path.basename(file_path)}_{i}"
                        documents.append(chunk)
                        metadatas.append({"source": file_path})
                        ids.append(doc_id)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {e}")

        if documents:
            print(f"üì¶ –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º {len(documents)} —á–∞–Ω–∫–æ–≤ –≤ ChromaDB...")
            embeddings = self.model.encode(documents).tolist()
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤")
        else:
            print("‚ùå –ù–ï–¢ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å –ø–∞–ø–∫—É data/docs")
    

    def _split_text(self, text: str, chunk_size: int = 300) -> List[str]:
        words = text.split()
        return [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

    def search(self, query: str, n_results: int = 3) -> List[str]:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query]).tolist()

        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=n_results
        )

        return results["documents"][0] if results["documents"] else []
